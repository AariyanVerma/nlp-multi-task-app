# -*- coding: utf-8 -*-
"""NLP FINAL PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MA4r-aUT9gIelxLe1smdb-FS_w-JEiat

#**CORONA_NLP DTASET**
"""

!pip install -q nltk
import pandas as pd
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import re

df = pd.read_csv('Corona_NLP_test.csv', encoding='latin1')

print("Dataset Shape:", df.shape)

df.head()

"""**Data cleaning and pre-processing**"""

stop_words = set(stopwords.words('english'))
def preprocess(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    text = re.sub(r'\@w+|\#','', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.isalpha()]
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

df['OriginalTweet'] = df['OriginalTweet'].astype(str)
df['cleaned_text'] = df['OriginalTweet'].apply(preprocess)
df[['OriginalTweet', 'cleaned_text']].head()

"""**Encode Labels & Split Dataset**"""

from sklearn.model_selection import train_test_split

df['Sentiment'] = df['Sentiment'].map({
    'Extremely Negative': 'Negative',
    'Negative': 'Negative',
    'Neutral': 'Neutral',
    'Positive': 'Positive',
    'Extremely Positive': 'Positive'
})

df = df[df['Sentiment'].isin(['Positive', 'Negative', 'Neutral'])]
X = df['cleaned_text']
y = df['Sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**TF-IDF Vectorization**"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

"""**Model Training â€“ Logistic Regression**"""

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_vec, y_train)

"""**Evaluation**"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = lr_model.predict(X_test_vec)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

"""**Sentiment Prediction Function**"""

def predict_sentiment(text):
    processed = preprocess(text)
    vec = vectorizer.transform([processed])
    prediction = lr_model.predict(vec)[0]
    return prediction

sample_tweet = "The new vaccine rollout has been very effective!"
print("Tweet:", sample_tweet)
print("Predicted Sentiment:", predict_sentiment(sample_tweet))

"""#**SMS-SPAM DATASET**"""

df_sms = pd.read_csv('SMS Spam Collection Dataset.csv', encoding='latin1')

print("Shape:", df_sms.shape)

df_sms.columns = ['label', 'message','C','D','E']

df_sms.head()

"""**Encode Labels & Preprocess Messages**"""

stop_words = set(stopwords.words('english'))

def preprocess_sms(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"\d+", "", text)
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w.isalpha()]
    tokens = [w for w in tokens if w not in stop_words]
    return ' '.join(tokens)

df_sms['cleaned_text'] = df_sms['message'].apply(preprocess_sms)

df_sms['label'] = df_sms['label'].map({'ham': 0, 'spam': 1})
df_sms.head()

"""**Train-Test Split**"""

from sklearn.model_selection import train_test_split

X_sms = df_sms['cleaned_text']
y_sms = df_sms['label']

X_sms_train, X_sms_test, y_sms_train, y_sms_test = train_test_split(X_sms, y_sms, test_size=0.2, random_state=42)

"""**Vectorize SMS Text**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_sms = TfidfVectorizer(max_features=5000)
X_sms_train_vec = tfidf_sms.fit_transform(X_sms_train)
X_sms_test_vec = tfidf_sms.transform(X_sms_test)

"""**Train Naive Bayes Classifier**"""

from sklearn.naive_bayes import MultinomialNB
nb_sms_model = MultinomialNB()
nb_sms_model.fit(X_sms_train_vec, y_sms_train)

"""**Evaluate Model**"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_sms_pred = nb_sms_model.predict(X_sms_test_vec)

print("Accuracy:", accuracy_score(y_sms_test, y_sms_pred))
print("Confusion Matrix:\n", confusion_matrix(y_sms_test, y_sms_pred))
print("Classification Report:\n", classification_report(y_sms_test, y_sms_pred))

"""**Real-Time Spam Detector Function**"""

def predict_spam(text):
    cleaned = preprocess_sms(text)
    vec = tfidf_sms.transform([cleaned])
    pred = nb_sms_model.predict(vec)[0]
    return "SPAM" if pred == 1 else "HAM (Not Spam)"

sample_msg = "Congratulations! You've won a $500 Walmart gift card. Claim now!"
print("Message:", sample_msg)
print("Prediction:", predict_spam(sample_msg))

"""#**Fake News Dataset**"""

df_fake = pd.read_csv('Fake.csv')

print("Shape:", df_fake.shape)

df_fake.head()

"""**Preprocessing**"""

stop_words = set(stopwords.words('english'))

def preprocess_fake(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"\d+", "", text)
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w.isalpha()]
    tokens = [w for w in tokens if w not in stop_words]
    return ' '.join(tokens)

df_fake['text'] = df_fake['text'].fillna("")
df_fake['cleaned_text'] = df_fake['text'].apply(preprocess_fake)
df_fake['label'] = 1  # 1 = Fake

df_fake[['text', 'cleaned_text']].head()

"""**Pre-Processing**"""

df_fake = df_fake[['cleaned_text', 'label']]

df_real = df_fake.sample(1000, random_state=42).copy()
df_real['label'] = 0  # 0 = Real
df_news = pd.concat([df_fake, df_real], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)

X_news = df_news['cleaned_text']
y_news = df_news['label']

"""**Vectorization (TF-IDF)**"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_news = TfidfVectorizer(max_features=5000)
X_news_vec = tfidf_news.fit_transform(X_news)

"""**Train/Test Split + Model Training**"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news_vec, y_news, test_size=0.2, random_state=42)

news_model = LogisticRegression(max_iter=1000)
news_model.fit(X_train_news, y_train_news)

"""**Evaluation**"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_pred_news = news_model.predict(X_test_news)

print("Accuracy:", accuracy_score(y_test_news, y_pred_news))
print("Confusion Matrix:\n", confusion_matrix(y_test_news, y_pred_news))
print("Classification Report:\n", classification_report(y_test_news, y_pred_news))

"""**Real-Time Fake News Detection Function**"""

def predict_fake_news(text):
    cleaned = preprocess_fake(text)
    vec = tfidf_news.transform([cleaned])
    pred = news_model.predict(vec)[0]
    return "FAKE" if pred == 1 else "REAL"

sample_article = "The government just approved a new vaccine for global rollout."
print("Article:", sample_article)
print("Prediction:", predict_fake_news(sample_article))

"""#**AMAZON REVIEWS SENTIMENT ANALYSIS DATASET**"""

def read_fasttext_file(filepath):
    texts, labels = [], []
    with open(filepath, 'r', encoding='utf-8') as file:
        for line in file:
            parts = line.strip().split(' ', 1)
            if len(parts) == 2:
                label = parts[0].replace('__label__', '')
                text = parts[1]
                texts.append(text)
                labels.append(label)
    return pd.DataFrame({'label': labels, 'text': texts})

df_amazon = read_fasttext_file('/content/test.ft.txt')

print("Shape:", df_amazon.shape)

df_amazon.head()

"""**Preprocess Amazon Review Text**"""

stop_words = set(stopwords.words('english'))

def preprocess_amazon_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.isalpha()]
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

df_amazon['cleaned_text'] = df_amazon['text'].apply(preprocess_amazon_text)

"""**Train-Test Split**"""

from sklearn.model_selection import train_test_split

X_amazon = df_amazon['cleaned_text']
y_amazon = df_amazon['label']

X_train_amz, X_test_amz, y_train_amz, y_test_amz = train_test_split(X_amazon, y_amazon, test_size=0.2, random_state=42)

"""**TF-IDF Vectorization**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_amazon = TfidfVectorizer(max_features=5000)
X_train_amz_vec = tfidf_amazon.fit_transform(X_train_amz)
X_test_amz_vec = tfidf_amazon.transform(X_test_amz)

"""**Train Logistic Regression Model**"""

from sklearn.linear_model import LogisticRegression

amazon_model = LogisticRegression(max_iter=1000)
amazon_model.fit(X_train_amz_vec, y_train_amz)

"""**Evaluate Model**"""

from sklearn.metrics import classification_report, accuracy_score

y_pred_amz = amazon_model.predict(X_test_amz_vec)

print("Accuracy:", accuracy_score(y_test_amz, y_pred_amz))
print("Classification Report:\n", classification_report(y_test_amz, y_pred_amz))

"""**Real-Time Amazon Review Sentiment Prediction**"""

label_map = {
    '1': 'Negative',
    '2': 'Positive'
}

def predict_amazon_sentiment(text):
    cleaned = preprocess_amazon_text(text)
    vec = tfidf_amazon.transform([cleaned])
    pred = amazon_model.predict(vec)[0]
    return label_map.get(pred, "Unknown Sentiment")

sample_review = "The product quality is decent but the delivery was delayed."
print("Review:", sample_review)
print("Predicted Sentiment:", predict_amazon_sentiment(sample_review))

"""#**NEWS CLASSIFICATION DATASET**"""

df_news5 = pd.read_csv('/content/test.csv')

print("Shape:", df_news5.shape)

df_news5.head()

"""**Preprocess Text**"""

stop_words = set(stopwords.words('english'))

def preprocess_news_desc(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w.isalpha()]
    tokens = [w for w in tokens if w not in stop_words]
    return ' '.join(tokens)

df_news5['cleaned_text'] = df_news5['Description'].apply(preprocess_news_desc)

"""**Prepare Features and Labels**"""

from sklearn.model_selection import train_test_split

X_news5 = df_news5['cleaned_text']
y_news5 = df_news5['Class Index']

X_train5, X_test5, y_train5, y_test5 = train_test_split(X_news5, y_news5, test_size=0.2, random_state=42)

"""**TF-IDF Vectorization**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_news5 = TfidfVectorizer(max_features=5000)
X_train5_vec = tfidf_news5.fit_transform(X_train5)
X_test5_vec = tfidf_news5.transform(X_test5)

"""**Train Classification Model**"""

from sklearn.linear_model import LogisticRegression

news5_model = LogisticRegression(max_iter=1000)
news5_model.fit(X_train5_vec, y_train5)

"""**Evaluation**"""

from sklearn.metrics import classification_report, accuracy_score

y_pred5 = news5_model.predict(X_test5_vec)

print("Accuracy:", accuracy_score(y_test5, y_pred5))
print("Classification Report:\n", classification_report(y_test5, y_pred5))

df_news5[['Class Index', 'Title']].groupby('Class Index').first()

class_map = {
    1: 'Social Issues / Politics',
    2: 'Sports',
    3: 'Finance / Economy',
    4: 'Science & Technology'
}

"""**Real-Time News Category Prediction Function**"""

def predict_news_category_task5(text):
    cleaned = preprocess_news_desc(text)
    vec = tfidf_news5.transform([cleaned])
    pred_index = news5_model.predict(vec)[0]
    return class_map.get(pred_index, "Unknown Category")

sample_text = "Apple unveils new M3 chip with faster processing speed."
print("Predicted Category:", predict_news_category_task5(sample_text))

"""#**Unified NLP Engine**

**Task Dispatcher Function**
"""

def predict_task(text, task):
    if task == 'corona_sentiment':
        cleaned = preprocess(text)
        vec = vectorizer.transform([cleaned])
        pred = lr_model.predict(vec)[0]
        return f"[Corona Sentiment] â†’ {pred}"

    elif task == 'sms_spam':
        cleaned = preprocess_sms(text)
        vec = tfidf_sms.transform([cleaned])
        pred = nb_sms_model.predict(vec)[0]
        return "[Spam Detection] â†’ SPAM" if pred == 1 else "[Spam Detection] â†’ HAM (Not Spam)"

    elif task == 'fake_news':
        cleaned = preprocess_fake(text)
        vec = tfidf_news.transform([cleaned])
        pred = news_model.predict(vec)[0]
        return "[Fake News Detection] â†’ FAKE" if pred == 1 else "[Fake News Detection] â†’ REAL"

    elif task == 'amazon_sentiment':
        cleaned = preprocess_amazon_text(text)
        vec = tfidf_amazon.transform([cleaned])
        pred = amazon_model.predict(vec)[0]
        label_map_amazon = {
            '1': 'Negative',
            '2': 'Positive'
        }
        return f"[Amazon Review Sentiment] â†’ {label_map_amazon.get(pred, 'Unknown')}"

    elif task == 'news_category':
        cleaned = preprocess_news_desc(text)
        vec = tfidf_news5.transform([cleaned])
        pred_index = news5_model.predict(vec)[0]
        class_map = {
            1: 'Social Issues / Law',
            2: 'Sports',
            3: 'Finance / Economy',
            4: 'Science & Technology'
        }
        return f"[News Classification] â†’ {class_map.get(pred_index, 'Unknown Category')}"

    else:
        return "Invalid task name. Please choose a valid task."

"""**Testing**"""

# Test examples
inputs = [
    ("Iâ€™m really happy with the new phone. Battery lasts all day!", "amazon_sentiment"),
    ("Congratulations! Youâ€™ve won a $500 gift card.", "sms_spam"),
    ("Scientists reveal breakthrough in quantum computing", "news_category"),
    ("This product broke within two days of use. Not satisfied.", "amazon_sentiment"),
    ("Cases are rising again in the US. People are scared.", "corona_sentiment"),
    ("This article claims aliens built the pyramids...", "fake_news")
]

# Run predictions
for text, task in inputs:
    print("\nInput:", text)
    print(predict_task(text, task))